{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4b88bf",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "- Introduction\n",
    "- problem Statement\n",
    "- Objectives\n",
    "- Code:\n",
    "    - Installing & Importing Packages\n",
    "    - Downloading & Reading Data\n",
    "    - Exploratory Data Analysis\n",
    "    - Data Preprocessing\n",
    "    - Feature Extraction\n",
    "    - Feature Importance\n",
    "    - Feature Selection\n",
    "    - Modeling\n",
    "    - Evaluation\n",
    "    - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54302c",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21437f9f",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6323f4",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2604d",
   "metadata": {},
   "source": [
    "## Code:\n",
    "### 1) Installing Needed Packages\n",
    "\n",
    "You might need to restart the kernel after installing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3a0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "# !pip install pandas-profiling\n",
    "# !pip install yfinance\n",
    "# !pip install skforecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d49ab8",
   "metadata": {},
   "source": [
    "### 2) Importing Needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e68668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arch\n",
      "  Downloading arch-5.3.1-cp38-cp38-win_amd64.whl (845 kB)\n",
      "Requirement already satisfied: statsmodels>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from arch) (0.13.2)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from arch) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from arch) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from arch) (1.7.3)\n",
      "Collecting property-cached>=1.6.4\n",
      "  Downloading property_cached-1.6.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0->arch) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0->arch) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0->arch) (1.16.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from statsmodels>=0.11->arch) (0.5.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from statsmodels>=0.11->arch) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=21.3->statsmodels>=0.11->arch) (3.0.4)\n",
      "Installing collected packages: property-cached, arch\n",
      "Successfully installed arch-5.3.1 property-cached-1.6.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc91e7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skforecast'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsaplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_acf, plot_pacf\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mForecasterAutoreg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForecasterAutoreg\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01march\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01march\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arch_model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skforecast'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "from ipywidgets import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "import arch\n",
    "from arch import arch_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b7674",
   "metadata": {},
   "source": [
    "### 3) Downloading & Reading Datasets\n",
    "- Downloading datasets from yahoo finance and reading the rest\n",
    "- Adjusting the needed columns types from the datasets and choosing the final columns to be used\n",
    "- Defining the set of rows that we will include in our analysis (Dates from 2/1/1990 till 2/11/2022) \n",
    "- Merging all datasets to form the final **df** which will be the dataset of our problem\n",
    "\n",
    "\n",
    "#### TO DO: Make a function to read data and output the merged dataset instead of repeating the steps for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5557cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This may take a long time depending on network bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53269d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Treasury Yield 10 Years (^TNX)\n",
    "TNX = yf.download(\"^TNX\", start='1990-01-01')\n",
    "TNX.to_csv('TNX.csv')\n",
    "TNX = pd.read_csv('TNX.csv')\n",
    "TNX['Date'] = pd.to_datetime(TNX['Date'])\n",
    "TNX = TNX[['Date', 'Adj Close']]\n",
    "TNX.columns = ['Date', 'TNX_Close']\n",
    "TNX = TNX[(TNX['Date']>='1990-01-02') &(TNX['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90da20a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# HANG SENG INDEX (^HSI)\n",
    "HSI = yf.download(\"^HSI\", start='1990-01-01')\n",
    "HSI.to_csv('HSI.csv')\n",
    "HSI = pd.read_csv('HSI.csv')\n",
    "HSI['Date'] = pd.to_datetime(HSI['Date'])\n",
    "HSI = HSI[['Date', 'Adj Close']]\n",
    "HSI.columns = ['Date', 'HSI_Close']\n",
    "HSI = HSI[(HSI['Date']>='1990-01-02') &(HSI['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f252b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# KOSPI Composite Index (^KS11)\n",
    "VKOSPI = yf.download(\"^KS11\", start='1990-01-01')\n",
    "VKOSPI.to_csv('VKOSPI.csv')\n",
    "VKOSPI = pd.read_csv('VKOSPI.csv')\n",
    "VKOSPI['Date'] = pd.to_datetime(VKOSPI['Date'])\n",
    "VKOSPI = VKOSPI[['Date', 'Adj Close']]\n",
    "VKOSPI.columns = ['Date', 'VKOSPI_Close']\n",
    "VKOSPI = VKOSPI[(VKOSPI['Date']>='1990-01-02') &(VKOSPI['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "582ac2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# S&P 500 (^GSPC)\n",
    "GSPC = yf.download(\"^GSPC\", start='1990-01-01')\n",
    "GSPC.to_csv('GSPC.csv')\n",
    "GSPC = pd.read_csv('GSPC.csv')\n",
    "GSPC['Date'] = pd.to_datetime(GSPC['Date'])\n",
    "GSPC = GSPC[['Date', 'Adj Close']]\n",
    "GSPC.columns = ['Date', 'GSPC_Close']\n",
    "GSPC = GSPC[(GSPC['Date']>='1990-01-02') &(GSPC['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1656374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# S&P-GSCI Commodity Index Future\n",
    "GSCI = yf.download(\"^SPGSCI\", start='1990-01-01')\n",
    "GSCI.to_csv('GSCI.csv')\n",
    "GSCI = pd.read_csv('GSCI.csv')\n",
    "GSCI['Date'] = pd.to_datetime(GSCI['Date'])\n",
    "GSCI = GSCI[['Date', 'Adj Close']]\n",
    "GSCI.columns = ['Date', 'GSCI_Close']\n",
    "GSCI = GSCI[(GSCI['Date']>='1990-01-02') &(GSCI['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b4863de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# CBOE Volatility Index (^VIX)\n",
    "VIX = yf.download(\"^VIX\", start='1990-01-01')\n",
    "VIX.to_csv('VIX.csv')\n",
    "VIX = pd.read_csv('VIX.csv')\n",
    "VIX['Date'] = pd.to_datetime(VIX['Date'])\n",
    "VIX = VIX[['Date', 'Adj Close']]\n",
    "VIX.columns = ['Date', 'VIX_Close']\n",
    "VIX = VIX[(VIX['Date']>='1990-01-02') &(VIX['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4591966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# No yahoo fin download - waiting on GREG\n",
    "CPI = pd.read_csv('CPI Data.csv')\n",
    "CPI['Date'] = pd.to_datetime(CPI['Date'])\n",
    "CPI = CPI[['Date', 'Adj Close']]\n",
    "CPI.columns = ['Date', 'CPI_Close']\n",
    "CPI = CPI[(CPI['Date']>='1990-01-02') &(CPI['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4442d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# US Dollar/USDX - Index - Cash (DX-Y.NYB)\n",
    "doll_ind = yf.download(\"DX-Y.NYB\", start='1990-01-01')\n",
    "doll_ind.to_csv('Dollar Index.csv')\n",
    "doll_ind = pd.read_csv('Dollar Index.csv')\n",
    "doll_ind['Date'] = pd.to_datetime(doll_ind['Date'])\n",
    "doll_ind = doll_ind[['Date', 'Adj Close']]\n",
    "doll_ind.columns = ['Date', 'Dollar_Close']\n",
    "doll_ind = doll_ind[(doll_ind['Date']>='1990-01-02') &(doll_ind['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bf8bc26",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GDP.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6504\\2286826020.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# HOW IS THIS created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mGDP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GDP.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mGDP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DATE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGDP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DATE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mGDP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GDP_Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mGDP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGDP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGDP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;34m'1990-01-02'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGDP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;34m'2022-11-02'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GDP.csv'"
     ]
    }
   ],
   "source": [
    "# HOW IS THIS created - waiting on GREG\n",
    "GDP = pd.read_csv('GDP.csv')\n",
    "GDP['DATE'] = pd.to_datetime(GDP['DATE'])\n",
    "GDP.columns = ['Date', 'GDP_Close']\n",
    "GDP = GDP[(GDP['Date']>='1990-01-02') &(GDP['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc791d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#  HOW is this created download? \n",
    "EPU = pd.read_csv('EPU.csv')\n",
    "EPU['Date'] = pd.to_datetime(EPU['Date'])\n",
    "EPU = EPU[['Date', 'Adj Close']]\n",
    "EPU.columns = ['Date', 'EPU_Close']\n",
    "EPU = EPU[(EPU['Date']>='1990-01-02') &(EPU['Date']<='2022-11-02')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635409d7",
   "metadata": {},
   "source": [
    "#### Merging all data into the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173bc335",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VIX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\u001b[43mVIX\u001b[49m, TNX,  how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], right_on \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df, doll_ind,  how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], right_on \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df, CPI,  how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], right_on \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VIX' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.merge(VIX, TNX,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "df = pd.merge(df, doll_ind,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "df = pd.merge(df, CPI,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "# df = pd.merge(df, GDP,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "df = pd.merge(df, GSCI,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "df = pd.merge(df, EPU,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "df = pd.merge(df, GSPC,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "df = pd.merge(df, VKOSPI,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "df = pd.merge(df, HSI,  how='left', left_on=['Date'], right_on = ['Date'])\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97666575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>VIX_Close</th>\n",
       "      <th>TNX_Close</th>\n",
       "      <th>Dollar_Close</th>\n",
       "      <th>CPI_Close</th>\n",
       "      <th>GSCI_Close</th>\n",
       "      <th>EPU_Close</th>\n",
       "      <th>GSPC_Close</th>\n",
       "      <th>VKOSPI_Close</th>\n",
       "      <th>HSI_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>7.940</td>\n",
       "      <td>94.290001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>212.089996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>359.690002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2838.100098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-01-03</td>\n",
       "      <td>18.190001</td>\n",
       "      <td>7.990</td>\n",
       "      <td>94.419998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.639999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>358.760010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2858.699951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-01-04</td>\n",
       "      <td>19.219999</td>\n",
       "      <td>7.980</td>\n",
       "      <td>92.519997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>212.139999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>355.670013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2868.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-01-05</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>7.990</td>\n",
       "      <td>92.849998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.919998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352.200012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2839.899902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-01-08</td>\n",
       "      <td>20.260000</td>\n",
       "      <td>8.020</td>\n",
       "      <td>92.050003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353.790009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2816.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8270</th>\n",
       "      <td>2022-10-27</td>\n",
       "      <td>27.389999</td>\n",
       "      <td>3.937</td>\n",
       "      <td>110.589996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>643.830017</td>\n",
       "      <td>26.549999</td>\n",
       "      <td>3807.300049</td>\n",
       "      <td>2288.780029</td>\n",
       "      <td>15427.940430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>2022-10-28</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>4.010</td>\n",
       "      <td>110.669998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>636.530029</td>\n",
       "      <td>26.469999</td>\n",
       "      <td>3901.060059</td>\n",
       "      <td>2268.399902</td>\n",
       "      <td>14863.059570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8272</th>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>25.879999</td>\n",
       "      <td>4.077</td>\n",
       "      <td>111.529999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>636.840027</td>\n",
       "      <td>26.270000</td>\n",
       "      <td>3871.979980</td>\n",
       "      <td>2293.610107</td>\n",
       "      <td>14687.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8273</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>25.809999</td>\n",
       "      <td>4.052</td>\n",
       "      <td>111.480003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>641.530029</td>\n",
       "      <td>27.330000</td>\n",
       "      <td>3856.100098</td>\n",
       "      <td>2335.219971</td>\n",
       "      <td>15455.269531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8274</th>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>25.860001</td>\n",
       "      <td>4.059</td>\n",
       "      <td>111.349998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649.270020</td>\n",
       "      <td>26.540001</td>\n",
       "      <td>3759.689941</td>\n",
       "      <td>2336.870117</td>\n",
       "      <td>15827.169922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8275 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  VIX_Close  TNX_Close  Dollar_Close  CPI_Close  GSCI_Close  \\\n",
       "0    1990-01-02  17.240000      7.940     94.290001        NaN  212.089996   \n",
       "1    1990-01-03  18.190001      7.990     94.419998        NaN  215.639999   \n",
       "2    1990-01-04  19.219999      7.980     92.519997        NaN  212.139999   \n",
       "3    1990-01-05  20.110001      7.990     92.849998        NaN  206.919998   \n",
       "4    1990-01-08  20.260000      8.020     92.050003        NaN  199.750000   \n",
       "...         ...        ...        ...           ...        ...         ...   \n",
       "8270 2022-10-27  27.389999      3.937    110.589996        NaN  643.830017   \n",
       "8271 2022-10-28  25.750000      4.010    110.669998        NaN  636.530029   \n",
       "8272 2022-10-31  25.879999      4.077    111.529999        NaN  636.840027   \n",
       "8273 2022-11-01  25.809999      4.052    111.480003        NaN  641.530029   \n",
       "8274 2022-11-02  25.860001      4.059    111.349998        NaN  649.270020   \n",
       "\n",
       "      EPU_Close   GSPC_Close  VKOSPI_Close     HSI_Close  \n",
       "0           NaN   359.690002           NaN   2838.100098  \n",
       "1           NaN   358.760010           NaN   2858.699951  \n",
       "2           NaN   355.670013           NaN   2868.000000  \n",
       "3           NaN   352.200012           NaN   2839.899902  \n",
       "4           NaN   353.790009           NaN   2816.000000  \n",
       "...         ...          ...           ...           ...  \n",
       "8270  26.549999  3807.300049   2288.780029  15427.940430  \n",
       "8271  26.469999  3901.060059   2268.399902  14863.059570  \n",
       "8272  26.270000  3871.979980   2293.610107  14687.019531  \n",
       "8273  27.330000  3856.100098   2335.219971  15455.269531  \n",
       "8274  26.540001  3759.689941   2336.870117  15827.169922  \n",
       "\n",
       "[8275 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddfc4d71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8275 entries, 0 to 8274\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Date          8275 non-null   datetime64[ns]\n",
      " 1   VIX_Close     8275 non-null   float64       \n",
      " 2   TNX_Close     8243 non-null   float64       \n",
      " 3   Dollar_Close  8273 non-null   float64       \n",
      " 4   CPI_Close     1 non-null      float64       \n",
      " 5   GSCI_Close    8274 non-null   float64       \n",
      " 6   EPU_Close     3367 non-null   float64       \n",
      " 7   GSPC_Close    8275 non-null   float64       \n",
      " 8   VKOSPI_Close  6186 non-null   float64       \n",
      " 9   HSI_Close     7912 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(9)\n",
      "memory usage: 711.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a10fe",
   "metadata": {},
   "source": [
    "### 4) Exploratory Data Analysis\n",
    "- Profile Reporting: Containing information about features: statistics and # number of missing values, etc.. and plots about distributions, interaction and correlation between features\n",
    "- Time Series Plots for each feature\n",
    "- Plots for target feature **VIX**; autocorrelation, seasonal decomposition, etc..\n",
    "- Analysis of the major historical incidents \n",
    "- Observations and steps to do in preprocessing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e87d4a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bf1aefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8345e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.to_file(\"EDA Report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50859e",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- CPI & GDP have around 90% missing values - should be analyzed\n",
    "- CPI & GDP are highly positively correlated - might need to drop one of them\n",
    "- TNX is highly negatively correlated with CPI & GDP - might need to drop one or two of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cc49c",
   "metadata": {},
   "source": [
    "#### Correlation of all features with VIX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corrwith(df[\"VIX_Close\"]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a06336",
   "metadata": {},
   "source": [
    "#### Time Series Plots for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4183ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function for plotting\n",
    "def plot_df(df, x, y, title=\"\", xlabel='Date', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    sns.lineplot(data=df,x=x, y=y)\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827d6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_df(df, 'Date', 'VIX_Close', title=\"Daily VIX Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'Dollar_Close', title=\"Daily Dollar Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'TNX_Close', title=\"Daily TNX Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'CPI_Close', title=\"Daily CPI Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'GDP_Close', title=\"Daily GDP Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'GSCI_Close', title=\"Daily GSCI Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'EPU_Close', title=\"Daily EPU Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'GSPC_Close', title=\"Daily GSPC Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'VKOSPI_Close', title=\"Daily VKOSPI Values\", xlabel='Date', ylabel='Value', dpi=100)\n",
    "plot_df(df, 'Date', 'HSI_Close', title=\"Daily HSI Values\", xlabel='Date', ylabel='Value', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070cf98",
   "metadata": {},
   "source": [
    "#### Plotting ACF and PACF for VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e63446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_acf(new_df.VIX_Close.tolist(), lags=30)\n",
    "plot_pacf(new_df.VIX_Close.tolist(), lags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af8335",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "Based on ACF and PACF plots, we can see that the first lag has a very high correlation to the current VIX values. Lag 2 has also correlation, however not a large one like the first lag. After that, we can see a decreasing correlation to further lags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43ff4b",
   "metadata": {},
   "source": [
    "### 5) Data Preprocessing\n",
    "- Dealing with missing data\n",
    "- Dealing with missing timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c677c2",
   "metadata": {},
   "source": [
    "#### Dealing with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab19619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 1st phase of filling missing values using forward fill\n",
    "new_df = df\n",
    "new_df.index = new_df['Date']\n",
    "new_df.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ed380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjusting the frequency to be daily because some days are missing \n",
    "new_df = new_df.asfreq(\"D\")\n",
    "new_df.drop(columns='Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After adjusting the frequency extra rows were created with the missing dates with missing values \n",
    "## Filling the generated missing values using the forward fill then the remaining missing values to be filled with zeros\n",
    "new_df.ffill(inplace=True)\n",
    "new_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c0f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d9bb90",
   "metadata": {},
   "source": [
    "##### Extracting Final data file to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv(\"Final Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f7f66",
   "metadata": {},
   "source": [
    "##### Reading Final Data instead of running the previous steps again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23085f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"Final Data.csv\")\n",
    "new_df['Date'] = pd.to_datetime(new_df['Date'])\n",
    "new_df.index = new_df['Date']\n",
    "new_df = new_df.asfreq(\"D\")\n",
    "# new_df.set_index(new_df['Date'],drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3eb77b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf2efe",
   "metadata": {},
   "source": [
    "#### Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b49c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to fill missing values first\n",
    "\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# from matplotlib import pyplot\n",
    "# # df.index = df['Date']\n",
    "# result = seasonal_decompose(new_df['VIX_Close'], model='multiplicative')\n",
    "# result.plot()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848e135",
   "metadata": {},
   "source": [
    "### 6) Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date-related Features\n",
    "\n",
    "new_df['year']=new_df.index.year \n",
    "new_df['month']=new_df.index.month \n",
    "new_df['day']=new_df.index.day\n",
    "new_df['dayofweek']=new_df.index.weekday\n",
    "new_df['weekofyear']=new_df.index.weekofyear\n",
    "new_df['quarter']=new_df.index.quarter\n",
    "new_df['month_start']=new_df.index.is_month_start\n",
    "new_df['month_end']=new_df.index.is_month_end\n",
    "new_df['quarter_start']=new_df.index.is_quarter_start\n",
    "new_df['quarter_end']=new_df.index.is_quarter_end\n",
    "new_df['year_start']=new_df.index.is_year_start\n",
    "new_df['year_end']=new_df.index.is_year_end\n",
    "new_df['leap_year']=new_df.index.is_leap_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46d07b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lag-related Features (Target Dependent Features)\n",
    "new_df['lag_1'] = new_df['VIX_Close'].shift(1)\n",
    "new_df['lag_2'] = new_df['VIX_Close'].shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d40546",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rolling Features \n",
    "\n",
    "def one_week_rolling_feature_extratror(data, col):\n",
    "    ''' Returns dataframe appended with additional features: last week average, maximum, minimum & std'''\n",
    "    data[col+'_prev_week_mean'] = data[col].rolling(window=7).mean()\n",
    "    data[col+'_prev_week_max'] = data[col].rolling(window=7).max()\n",
    "    data[col+'_prev_week_min'] = data[col].rolling(window=7).min()\n",
    "    data[col+'_prev_week_std'] = data[col].rolling(window=7).std()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32271202",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['TNX_Close','Dollar_Close','CPI_Close','GDP_Close','GSCI_Close','EPU_Close','GSPC_Close','VKOSPI_Close','HSI_Close']\n",
    "for col in columns:\n",
    "    new_df = one_week_rolling_feature_extratror(new_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a67465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033f49a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4011c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Filling missing values\n",
    "new_df = new_df.bfill()\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('Final Data with Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.read_csv('Final Data with Features.csv')\n",
    "new_df['Date'] = pd.to_datetime(new_df['Date'])\n",
    "new_df.index = new_df.Date\n",
    "new_df.drop(columns = ['Date.1'], inplace=True)\n",
    "new_df = new_df.asfreq(\"D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6b0c8",
   "metadata": {},
   "source": [
    "### 7) Modeling:\n",
    "\n",
    "- Split data to train and test\n",
    "- Apply models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fab22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data into train-test\n",
    "## Testing only on last 7 days in datases\n",
    "# returns = 100 * new_df.VIX_Close.pct_change().dropna()\n",
    "\n",
    "train=new_df.loc['1990-01-03':'2021-12-31'] \n",
    "test=new_df.loc['2022-01-01':'2022-11-02'] \n",
    "\n",
    "# train_y=returns.loc[:'2021-12-31'] \n",
    "# test_y=returns.loc['2022-01-01':'2022-11-02'] \n",
    "# valid=new_df.loc['2020-01-01':'2022-11-02']\n",
    "\n",
    "print(\"Training data shape =\", train.shape, \"\\nTesting data shape =\", test.shape)\n",
    "# print(\"Training univariate data shape =\", train_y.shape, \"\\nTesting univariate data shape =\", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9fb52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(14, 4))\n",
    "train['VIX_Close'].plot(ax=ax, label='Training Data')\n",
    "test['VIX_Close'].plot(ax=ax, label='Testing Data')\n",
    "# valid['VIX_Close'].plot(ax=ax, label='Validation Data')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611d800",
   "metadata": {},
   "source": [
    "### Univariate Time Series Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8fba5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create and train forecaster for univariate time series using Random Forest Regressor\n",
    "\n",
    "rf_uni = ForecasterAutoreg(\n",
    "                regressor = RandomForestRegressor(random_state=123),\n",
    "                lags      = 365\n",
    "             )\n",
    "\n",
    "rf_uni.fit(y=train['VIX_Close'])\n",
    "rf_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating Predictions\n",
    "predictions = rf_uni.predict(steps=len(test))\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b21e5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plotting Actual Vs Predicted\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "train['VIX_Close'].plot(ax=ax, label='train')\n",
    "test['VIX_Close'].plot(ax=ax, label='test')\n",
    "predictions.plot(ax=ax, label='predictions')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating RMSE for predictions across actual\n",
    "\n",
    "rf_uni_rmse = np.sqrt(mean_squared_error(\n",
    "                y_true = test['VIX_Close'],\n",
    "                y_pred = predictions\n",
    "            ))\n",
    "\n",
    "print(f\"Test error (rmse): {rf_uni_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302537e2",
   "metadata": {},
   "source": [
    "### Univariate Time Series Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63035150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train forecaster for univariate time series using Random Forest Regressor\n",
    "\n",
    "svr_uni = ForecasterAutoreg(\n",
    "                regressor = SVR(),\n",
    "                lags      = 365\n",
    "             )\n",
    "\n",
    "svr_uni.fit(y=train['VIX_Close'])\n",
    "svr_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de69614",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating Predictions\n",
    "predictions = svr_uni.predict(steps=len(test))\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9476d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Actual Vs Predicted\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "train['VIX_Close'].plot(ax=ax, label='train')\n",
    "test['VIX_Close'].plot(ax=ax, label='test')\n",
    "predictions.plot(ax=ax, label='predictions')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b993c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Calculating RMSE for predictions across actual\n",
    "\n",
    "svr_uni_rmse = np.sqrt(mean_squared_error(\n",
    "                y_true = test['VIX_Close'],\n",
    "                y_pred = predictions\n",
    "            ))\n",
    "\n",
    "print(f\"Test error (rmse): {svr_uni_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b18830",
   "metadata": {},
   "source": [
    "### Multivariate Time Series Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bef4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df = new_df.copy()\n",
    "multi_df['multi_VIX'] = multi_df['VIX_Close'].shift(-1)\n",
    "multi_train = multi_df.loc['1990-01-03':'2021-12-31'] \n",
    "multi_test = multi_df.loc['2022-01-01':'2022-11-01'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and train forecaster for multivariate time series using Random Forest Regressor\n",
    "rf_multi = ForecasterAutoreg(\n",
    "                regressor = RandomForestRegressor(random_state=123),\n",
    "                lags      = 365\n",
    "             )\n",
    "\n",
    "rf_multi.fit(y=multi_train['multi_VIX'], exog=multi_train.drop(columns=['Date','multi_VIX']))\n",
    "rf_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating Predictions\n",
    "predictions = rf_multi.predict(steps=len(multi_test), exog=multi_test.drop(columns=['Date','multi_VIX']))\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Actual Vs Predicted\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "multi_train['multi_VIX'].plot(ax=ax, label='train')\n",
    "multi_test['multi_VIX'].plot(ax=ax, label='test')\n",
    "predictions.plot(ax=ax, label='predictions')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c006b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Calculating RMSE for predictions across actual\n",
    "\n",
    "rf_multi_rmse = np.sqrt(mean_squared_error(\n",
    "                y_true = multi_test['multi_VIX'],\n",
    "                y_pred = predictions\n",
    "            ))\n",
    "\n",
    "print(f\"Test error (rmse): {rf_multi_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b266df",
   "metadata": {},
   "source": [
    "### Multivariate Time Series Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "## Create and train forecaster for multivariate time series using Support Vector Regressor\n",
    "svr_multi = ForecasterAutoreg(\n",
    "                regressor = SVR(),\n",
    "                lags      = 365\n",
    "             )\n",
    "\n",
    "svr_multi.fit(y=multi_train['multi_VIX'], exog=multi_train.drop(columns=['Date','multi_VIX']))\n",
    "svr_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283497ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating Predictions\n",
    "predictions = svr_multi.predict(steps=len(multi_test), exog=multi_test.drop(columns=['Date','multi_VIX']))\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Actual Vs Predicted\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "multi_train['multi_VIX'].plot(ax=ax, label='train')\n",
    "multi_test['multi_VIX'].plot(ax=ax, label='test')\n",
    "predictions.plot(ax=ax, label='predictions')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating RMSE for predictions across actual\n",
    "\n",
    "svr_multi_rmse = np.sqrt(mean_squared_error(\n",
    "                y_true = multi_test['multi_VIX'],\n",
    "                y_pred = predictions\n",
    "            ))\n",
    "\n",
    "print(f\"Test error (rmse): {svr_multi_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Univariate','Multivariate'],index=['Random Forest','SVR'])\n",
    "results.loc['Random Forest','Univariate'] = round(rf_uni_rmse,3)\n",
    "results.loc['Random Forest','Multivariate'] = round(rf_multi_rmse,3)\n",
    "results.loc['SVR','Univariate'] = round(svr_uni_rmse,3)\n",
    "results.loc['SVR','Multivariate'] = round(svr_multi_rmse,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3799fd",
   "metadata": {},
   "source": [
    "### Testing to See If Our Prediction Has Any Value Beyond Using Yesterday's Close to Predict Today's Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.sqrt(((multi_test['multi_VIX'] - multi_test['VIX_Close']) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd55d5a",
   "metadata": {},
   "source": [
    "### Answer = No, It Doesn't.  RMSE on our baseline test is lower than our multivariate model :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5ac4c",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "We can see that the multivariate Random Forest model is the best performing model which we will check the feature importance for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f6976",
   "metadata": {},
   "source": [
    "### Feature Importance \n",
    "- Top correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5adb8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_imp = rf_multi.get_feature_importance()\n",
    "\n",
    "feat_imp = feat_imp.sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=feat_imp[2:12]['feature'], x=feat_imp[2:12]['importance']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541ec30",
   "metadata": {},
   "source": [
    "### Univariate Time Series Using Arch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e15add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = 100 * new_df.GSPC_Close.pct_change().dropna()\n",
    "realized_vol = returns.rolling(5).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500f5e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(returns)\n",
    "plt.ylabel('Pct Return', fontsize=16)\n",
    "plt.title('S&P Volatility', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017903e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bic_arch = []\n",
    "\n",
    "for p in range(1, 5):\n",
    "    for q in range(1, 5):\n",
    "        arch = arch_model(returns, vol='ARCH', p=p, o=0, q=q)\\\n",
    "                .fit(last_obs=\"2021-12-31\",disp='off')\n",
    "        bic_arch.append(arch.bic)\n",
    "        if arch.bic == np.min(bic_arch):\n",
    "            best_param = p, q\n",
    "\n",
    "            \n",
    "arch = arch_model(returns, vol='ARCH', p=best_param[0], o=0, q=best_param[1])\\\n",
    "        .fit(last_obs=\"2021-12-31\",disp='off')\n",
    "\n",
    "print(best_param)\n",
    "print(arch.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6679e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = arch.forecast()\n",
    "forecast_arch = forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c70404",
   "metadata": {},
   "outputs": [],
   "source": [
    "realized_vol.iloc[-len(test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arch.variance.iloc[-len(test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "rmse_arch = np.sqrt(mse(realized_vol.iloc[-len(test):],np.sqrt(forecast_arch.variance.iloc[-len(test):] \n",
    "                         )))\n",
    "print('The RMSE value of ARCH model is {:.4f}'.format(rmse_arch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb44b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(realized_vol, label='Realized Volatility')\n",
    "plt.plot(np.sqrt(forecast_arch.variance.iloc[-len(test):]) , \n",
    "         label='Volatility Prediction-ARCH')\n",
    "plt.title('Volatility Prediction with ARCH', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b651c",
   "metadata": {},
   "source": [
    "### Univariate Time Series Using Garch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd343a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_garch = []\n",
    "\n",
    "for p in range(1, 5):\n",
    "    for q in range(1, 5):\n",
    "        garch = arch_model(returns, vol='GARCH', p=p, o=0, q=q)\\\n",
    "                .fit(last_obs=\"2021-12-31\",disp='off')\n",
    "        bic_garch.append(garch.bic)\n",
    "        if garch.bic == np.min(bic_garch):\n",
    "            best_param = p, q\n",
    "\n",
    "            \n",
    "garch = arch_model(returns, vol='GARCH', p=best_param[0], o=0, q=best_param[1])\\\n",
    "        .fit(last_obs=\"2021-12-31\",disp='off')\n",
    "print(best_param)\n",
    "print(garch.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = garch.forecast()\n",
    "forecast_garch = forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151184a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "rmse_garch = np.sqrt(mse(realized_vol.iloc[-len(test):],np.sqrt(forecast_garch.variance.iloc[-len(test):] \n",
    "                         )))\n",
    "print('The RMSE value of GARCH model is {:.4f}'.format(rmse_garch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(realized_vol, label='Realized Volatility')\n",
    "plt.plot(np.sqrt(forecast_garch.variance.iloc[-len(test):]) , \n",
    "         label='Volatility Prediction-GARCH')\n",
    "plt.title('Volatility Prediction with GARCH', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eaae06",
   "metadata": {},
   "source": [
    "### Trying ARCH models with Exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e88528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exog = new_df[1:]\n",
    "\n",
    "# exog = exog[['TNX_Close','Dollar_Close']]\n",
    "# exog.columns = ['x0','x1']\n",
    "\n",
    "# exog_fcast = test[['TNX_Close','Dollar_Close']]\n",
    "# exog_fcast.columns = ['x0','x1']\n",
    "\n",
    "# dict(exog_fcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b095f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# garch_exog = arch_model(y= returns, x = exog,\n",
    "#                         mean='ARX',lags=1, vol='GARCH', p=1, o=0, q=1).fit(last_obs=\"2021-12-31\",disp='off')\n",
    "# print(garch_exog.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_exog = garch_exog.forecast(start=\"2022-01-01\",horizon = 306, x=dict(exog_fcast))\n",
    "# forecast_garch_exog = forecast_exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1147191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_exog.variance.iloc[-len(test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error as mse\n",
    "# rmse_garch = np.sqrt(mse(realized_vol.iloc[-len(test):],np.sqrt(forecast_exog.variance.iloc[-len(test):] \n",
    "#                          )))\n",
    "# print('The RMSE value of GARCH model is {:.4f}'.format(rmse_garch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7371d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
